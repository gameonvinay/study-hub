import type { TheoryQuestion } from '../../../types'

export const theoryQuestion8: TheoryQuestion = {
  "id": 8,
  "title": "Explain the Text Summarization Techniques Used in NLP",
  "content": "## What is Text Summarization?\n\n**Text Summarization** is the process of creating a concise and coherent version of a longer document while preserving its key information and meaning.\n\n```\nText Summarization Overview:\n\n┌─────────────────────────────────────────────────────────────┐\n│                    Original Document                         │\n│    (Long text with multiple paragraphs and details)         │\n└───────────────────────────┬─────────────────────────────────┘\n                            │\n                            ▼\n              ┌─────────────────────────────┐\n              │    Summarization Algorithm   │\n              └─────────────────────────────┘\n                            │\n            ┌───────────────┴───────────────┐\n            ▼                               ▼\n    ┌───────────────┐               ┌───────────────┐\n    │  EXTRACTIVE   │               │  ABSTRACTIVE  │\n    │  (Select key  │               │  (Generate    │\n    │   sentences)  │               │   new text)   │\n    └───────────────┘               └───────────────┘\n```\n\n## Types of Text Summarization\n\n### 1. Extractive Summarization\n\n**Concept**: Selects and extracts important sentences directly from the original text without modification.\n\n```\nExtractive Summarization Process:\n\nOriginal Text:\n┌─────────────────────────────────────────────────────────┐\n│ Sentence 1: The weather was beautiful today.            │ Score: 0.3\n│ Sentence 2: Scientists discovered a new vaccine.        │ Score: 0.9 ★\n│ Sentence 3: The park was crowded with visitors.         │ Score: 0.2\n│ Sentence 4: The vaccine shows 95% effectiveness.        │ Score: 0.8 ★\n│ Sentence 5: Children were playing in the garden.        │ Score: 0.1\n└─────────────────────────────────────────────────────────┘\n                            │\n                            ▼\nSummary: \"Scientists discovered a new vaccine.\n          The vaccine shows 95% effectiveness.\"\n```\n\n**Techniques:**\n\n| Technique | Description | Method |\n|-----------|-------------|-------|\n| **TF-IDF** | Term Frequency-Inverse Document Frequency | Scores sentences by important word frequency |\n| **TextRank** | Graph-based ranking | Builds sentence similarity graph, uses PageRank |\n| **LexRank** | Graph-based with cosine similarity | Similar to TextRank with different similarity |\n| **LSA** | Latent Semantic Analysis | Uses SVD to find semantic topics |\n| **Sentence Scoring** | Position and length based | First/last sentences weighted higher |\n\n**TextRank Algorithm:**\n```\nTextRank Process:\n\nStep 1: Split text into sentences\nStep 2: Build similarity graph\n\n    S1 ──0.8── S2\n    │ ╲       ╱│\n   0.3 ╲0.5 ╱0.7\n    │   ╲ ╱   │\n    S4 ──0.6── S3\n\nStep 3: Apply PageRank algorithm\nStep 4: Select top-ranked sentences\n```\n\n---\n\n### 2. Abstractive Summarization\n\n**Concept**: Generates new sentences that capture the essence of the original text, similar to how humans summarize.\n\n```\nAbstractive Summarization Process:\n\nOriginal Text:\n\"The company reported a 20% increase in revenue\n compared to last year. The growth was primarily\n driven by strong sales in the Asian market.\"\n\n                    │\n                    ▼\n         ┌────────────────────┐\n         │  Seq2Seq Model     │\n         │  (Encoder-Decoder) │\n         └────────────────────┘\n                    │\n                    ▼\nGenerated Summary:\n\"Revenue grew 20% due to strong Asian sales.\"\n(New sentence not in original text)\n```\n\n**Techniques:**\n\n| Technique | Description |\n|-----------|------------|\n| **Sequence-to-Sequence (Seq2Seq)** | Encoder-decoder neural network architecture |\n| **Attention Mechanism** | Focuses on relevant parts of input when generating output |\n| **Transformer Models** | Self-attention based models (BERT, GPT, T5) |\n| **Pointer-Generator Networks** | Combines copying from source with generating new words |\n\n**Seq2Seq with Attention:**\n```\nEncoder-Decoder with Attention:\n\nInput: \"Scientists discovered a breakthrough vaccine for malaria\"\n\nENCODER                          DECODER\n┌─────────────────────┐         ┌─────────────────────┐\n│ Scientists          │         │                     │\n│     ↓               │   ←──   │  \"New\"              │\n│ discovered          │ Attention│     ↓               │\n│     ↓               │   ←──   │  \"vaccine\"          │\n│ breakthrough        │   ←──   │     ↓               │\n│     ↓               │         │  \"found\"            │\n│ vaccine             │         │                     │\n└─────────────────────┘         └─────────────────────┘\n\nOutput: \"New vaccine found for malaria\"\n```\n\n---\n\n## Comparison: Extractive vs Abstractive\n\n| Aspect | Extractive | Abstractive |\n|--------|------------|------------|\n| Output | Exact sentences from text | New generated sentences |\n| Fluency | Can be disjointed | More natural flow |\n| Accuracy | High factual accuracy | Risk of hallucination |\n| Complexity | Simpler to implement | More complex (DL required) |\n| Training Data | Minimal/None needed | Large datasets required |\n| Speed | Faster | Slower |\n\n---\n\n## Evaluation Metrics\n\n### 1. ROUGE (Recall-Oriented Understudy for Gisting Evaluation)\n\n```\nROUGE Metrics:\n\nROUGE-N: N-gram overlap\n┌─────────────────────────────────────────────────────────────┐\n│ Reference: \"The cat sat on the mat\"                         │\n│ Generated: \"The cat is on the mat\"                          │\n│                                                              │\n│ ROUGE-1 (unigrams): 5 matching / 6 reference = 0.83         │\n│ ROUGE-2 (bigrams):  3 matching / 5 reference = 0.60         │\n└─────────────────────────────────────────────────────────────┘\n\nROUGE-L: Longest Common Subsequence\n```\n\n### 2. BLEU (Bilingual Evaluation Understudy)\n\nMeasures precision of n-grams in generated text.\n\n### 3. Human Evaluation\n\n- Coherence\n- Relevance\n- Fluency\n- Informativeness\n\n---\n\n## NLP Libraries for Summarization\n\n| Library | Extractive | Abstractive |\n|---------|------------|------------|\n| NLTK | ✓ (basic) | ✗ |\n| Gensim | ✓ (TextRank) | ✗ |\n| Sumy | ✓ (multiple) | ✗ |\n| spaCy | ✓ | ✗ |\n| Hugging Face | ✓ | ✓ (T5, BART, GPT) |\n| TensorFlow | ✓ | ✓ |\n\n---\n\n## Applications\n\n- **News Aggregation**: Summarize multiple articles\n- **Document Review**: Legal, medical document summaries\n- **Search Engines**: Snippet generation\n- **Email Summarization**: Gmail's smart replies\n- **Meeting Notes**: Auto-generated meeting summaries\n- **Research**: Scientific paper abstracts"
}
