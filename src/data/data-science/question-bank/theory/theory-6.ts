import type { TheoryQuestion } from '../../../types'

export const theoryQuestion6: TheoryQuestion = {
  "id": 6,
  "title": "Explain the Different Types of Classifiers with Examples",
  "content": "**Classification** is a supervised learning technique that categorizes data into predefined classes. A **classifier** is an algorithm that implements classification.\n\n```\nClassification Overview:\n┌─────────────────────────────────────────────────────────────┐\n│                     CLASSIFICATION                           │\n├─────────────────────────────────────────────────────────────┤\n│                                                              │\n│   Input Data    ──▶   Classifier   ──▶   Predicted Class   │\n│   (Features)          Algorithm          (Label)            │\n│                                                              │\n│   Example:                                                   │\n│   [Age, Income,  ──▶   Decision    ──▶   \"Approved\" or     │\n│    Credit Score]       Tree              \"Rejected\"         │\n│                                                              │\n└─────────────────────────────────────────────────────────────┘\n```\n\n## 1. Decision Tree Classifier\n\n**Concept**: Creates a tree-like model of decisions based on feature values. Each internal node represents a test on a feature, each branch represents an outcome, and each leaf node represents a class label.\n\n```\nDecision Tree Example (Loan Approval):\n\n                    [Credit Score?]\n                    /            \\\n               < 600             >= 600\n                /                    \\\n          [Rejected]           [Income > 50K?]\n                                /           \\\n                              No             Yes\n                              /               \\\n                       [Rejected]         [Approved]\n```\n\n**Advantages:**\n- Easy to understand and interpret\n- Handles both numerical and categorical data\n- Requires little data preprocessing\n\n**Disadvantages:**\n- Can overfit with complex trees\n- Sensitive to small data changes\n\n**Example Use Case**: Credit card approval, Medical diagnosis\n\n---\n\n## 2. Random Forest Classifier\n\n**Concept**: An ensemble method that builds multiple decision trees and combines their predictions through voting.\n\n```\nRandom Forest:\n┌─────────────────────────────────────────────────────────────┐\n│                                                              │\n│   Tree 1      Tree 2      Tree 3     ...     Tree n         │\n│     │           │           │                  │            │\n│     ▼           ▼           ▼                  ▼            │\n│   Class A     Class B     Class A          Class A          │\n│                                                              │\n│              ┌─────────────────────┐                        │\n│              │   Majority Vote     │                        │\n│              │   = Class A         │                        │\n│              └─────────────────────┘                        │\n│                                                              │\n└─────────────────────────────────────────────────────────────┘\n```\n\n**Advantages:**\n- More accurate than single decision tree\n- Handles high-dimensional data\n- Reduces overfitting\n\n**Example Use Case**: Fraud detection, Customer churn prediction\n\n---\n\n## 3. Naive Bayes Classifier\n\n**Concept**: Based on Bayes' theorem with an assumption of independence between features.\n\n**Formula**: P(Class|Features) = P(Features|Class) × P(Class) / P(Features)\n\n**Types:**\n| Type | Data Type | Example |\n|------|-----------|--------|\n| Gaussian NB | Continuous | Height, weight |\n| Multinomial NB | Discrete counts | Word frequencies |\n| Bernoulli NB | Binary | Yes/No features |\n\n**Advantages:**\n- Fast and efficient\n- Works well with high-dimensional data\n- Good for text classification\n\n**Example Use Case**: Spam email detection, Sentiment analysis\n\n---\n\n## 4. K-Nearest Neighbors (KNN) Classifier\n\n**Concept**: Classifies data based on the majority class among its K nearest neighbors.\n\n```\nKNN Classification (K=3):\n\n              ★ (Class A)\n\n    ● ● ●                 ▲ ▲\n     (B)     [?]           (C)\n              ●              ▲\n\n\n    New point [?] → Look at 3 nearest → 2 are B, 1 is C → Classify as B\n```\n\n**Key Considerations:**\n- Choice of K (odd number preferred)\n- Distance metric (Euclidean, Manhattan)\n- Feature scaling is important\n\n**Advantages:**\n- Simple and intuitive\n- No training phase\n- Works well with small datasets\n\n**Disadvantages:**\n- Slow for large datasets\n- Sensitive to irrelevant features\n\n**Example Use Case**: Recommendation systems, Pattern recognition\n\n---\n\n## 5. Support Vector Machine (SVM)\n\n**Concept**: Finds the optimal hyperplane that maximizes the margin between classes.\n\n```\nSVM Concept:\n\n        Class A (●)                    Class B (▲)\n\n        ●    ●                              ▲\n                                       ▲\n        ●        ●    │   Margin   │     ▲\n                     ═══════════════\n        ●    ●       │  Hyperplane│        ▲    ▲\n                     ═══════════════\n             ●    ●  │            │    ▲\n                                          ▲\n\n    The hyperplane maximizes the distance (margin) between classes\n```\n\n**Kernel Types:**\n- Linear: For linearly separable data\n- RBF (Radial Basis Function): For non-linear data\n- Polynomial: For polynomial decision boundaries\n\n**Advantages:**\n- Effective in high dimensions\n- Memory efficient\n- Works well with clear margin of separation\n\n**Example Use Case**: Image classification, Text categorization\n\n---\n\n## 6. Logistic Regression\n\n**Concept**: Despite its name, it's a classification algorithm that uses the logistic (sigmoid) function to predict probabilities.\n\n```\nLogistic Function:\n                    1\n    P(y=1) = ─────────────────\n             1 + e^(-z)\n\nWhere z = w₀ + w₁x₁ + w₂x₂ + ... + wₙxₙ\n\nOutput: Probability between 0 and 1\nIf P(y=1) > 0.5 → Class 1\nIf P(y=1) ≤ 0.5 → Class 0\n```\n\n**Advantages:**\n- Outputs probabilities\n- Works well for linearly separable data\n- Easy to implement and interpret\n\n**Example Use Case**: Disease prediction, Customer conversion\n\n---\n\n## 7. Neural Network Classifier\n\n**Concept**: Inspired by biological neural networks, consists of interconnected nodes (neurons) organized in layers.\n\n```\nNeural Network Architecture:\n\n    Input Layer      Hidden Layer(s)     Output Layer\n\n        ○                 ○                  ○ (Class A)\n         ╲               ╱│╲               ╱\n        ○─────────────○───────────────○ (Class B)\n         ╱               ╲│╱               ╲\n        ○                 ○                  ○ (Class C)\n\n    Features          Processing          Prediction\n```\n\n**Advantages:**\n- Handles complex, non-linear relationships\n- Automatic feature extraction\n- State-of-the-art for many tasks\n\n**Disadvantages:**\n- Requires large amounts of data\n- Computationally expensive\n- Black box (hard to interpret)\n\n**Example Use Case**: Image recognition, Speech recognition\n\n---\n\n## Classifier Selection Guide\n\n| Scenario | Recommended Classifier |\n|----------|----------------------|\n| Small dataset, interpretability needed | Decision Tree |\n| Text classification | Naive Bayes |\n| High accuracy needed | Random Forest, Neural Network |\n| High-dimensional data | SVM |\n| Quick prototyping | Logistic Regression |\n| Non-linear relationships | Neural Network, SVM with RBF |"
}
